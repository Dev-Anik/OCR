<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>OCR</title>
  <script src="https://unpkg.com/tesseract.js@v3.0.3/dist/tesseract.min.js"></script>
  <style>
    #video, #canvas { width: 100%; height: auto; display:block; }
    #canvas { border:1px solid #ccc; }
  </style>
</head>
<body>
  <h1>OCR</h1>
  <video id="video" autoplay muted playsinline></video>
  <canvas id="canvas"></canvas>
  <p id="progress"></p>
  <p id="result"></p>

  <script>
    const videoElem = document.getElementById('video');
    const canvasElem = document.getElementById('canvas');
    const progressElem = document.getElementById('progress');
    const resultElem = document.getElementById('result');

    // offscreen buffer to capture the box region
    const buf = document.createElement('canvas');

    let isRecognizing = false;

    // Get camera
    navigator.mediaDevices.getUserMedia({ video: { facingMode: "environment" }, audio: false })
      .then(stream => {
        videoElem.srcObject = stream;

        // Wait until we know video dimensions
        videoElem.addEventListener('loadedmetadata', () => {
          // set canvas size to video size (you can scale to match display if needed)
          canvasElem.width = videoElem.videoWidth;
          canvasElem.height = videoElem.videoHeight;

          // define capture box (example: centered thin horizontal box)
          const box = {
            x: 50,
            h: 100
          };
          box.y = (canvasElem.height - box.h) / 2;
          box.w = (canvasElem.width - box.x * 2);

          // prepare buffer size
          buf.width = box.w;
          buf.height = box.h;

          const ctx = canvasElem.getContext('2d');
          const bufCtx = buf.getContext('2d');

          // single interval: draw video -> overlay box -> every N ms capture & OCR if not busy
          const intervalMs = 800; // slower OCR interval (tweak as needed)
          setInterval(async () => {
            // draw current video frame to visible canvas
            ctx.drawImage(videoElem, 0, 0, videoElem.videoWidth, videoElem.videoHeight,
              0, 0, canvasElem.width, canvasElem.height);

            // draw rectangle overlay
            ctx.beginPath();
            ctx.rect(box.x, box.y, box.w, box.h);
            ctx.lineWidth = 2;
            ctx.strokeStyle = 'red';
            ctx.stroke();

            if (isRecognizing) return; // already running OCR

            // copy the box region into buffer
            bufCtx.clearRect(0, 0, buf.width, buf.height);
            bufCtx.drawImage(canvasElem, box.x, box.y, box.w, box.h, 0, 0, box.w, box.h);

            isRecognizing = true;
            progressElem.textContent = 'recognizing...';

            try {
              const { data: { text } } = await Tesseract.recognize(buf, 'eng', {
                logger: m => {
                  // you can display percent or status messages here
                  progressElem.textContent = `${m.status}${m.progress ? ' ' + Math.round(m.progress*100) + '%' : ''}`;
                }
              });
              resultElem.textContent = text.trim();
            } catch (err) {
              resultElem.textContent = 'OCR error: ' + err.message;
            } finally {
              isRecognizing = false;
            }
          }, intervalMs);
        });
      })
      .catch(err => {
        resultElem.textContent = 'Camera error: ' + err.message;
      });
  </script>
</body>
</html>
